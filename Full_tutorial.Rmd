---
title: "Full tutorial"
author: "Danny"
date: "1/5/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This is meant to be a very general introduction for using the Rasch model to help construct measures and surveys in discipline based education research. This is meant to get you started but is by no means where you should stop. Please see the references section for where to go next. 



```{r include=FALSE}
# automatically create a bib database for R packages
library(knitr)
library(kableExtra)
library(TAM)
library(WrightMap)
```



# Installing R and R-Studio

## Instructions for installing R:
1. Go to this web page: [http://cran.stat.ucla.edu/](http://cran.stat.ucla.edu/)
2. Under the “Download and Install R” heading, select your operating system (Windows, Mac,
Linux).
3. The directions diverge at this stage, depending on your OS.  

### For Mac, do the following:
1. Under the “Latest Release” heading, select the top “.pkg” link. Save the file to your computer.
2. This is the basic installer file.

### For Windows do the following:
1. Under the “Subdirectories” heading, select the top “base” link. Save the file to your computer.
2. This is the basic installer file.

### For Linux, you already know what you’re doing. I’ll stay out of your way.

1. Download and open the installer file. Now, just follow the instructions to set up R. The default settings are fine. No need to open the program yet.
2. Now, we’re going to download R-Studio, which is the user interface that makes R faster and easier to use. It's an integrated development environment (IDE)
3. Once you have R-Studio, you won’t need to open the “base R” GUI anymore, since R-Studio does this for you.

## Instructions for installing R-Studio:
1. Go to this web page: [https://www.rstudio.com/products/rstudio/download/#download](https://www.rstudio.com/products/rstudio/download/#download)
2. Under the `Installers for Supported Platforms` header, select your operating system (Windows, Mac, Linux).
3. Download and open the installer file and follow the instructions. The default settings are fine.
4. Once R-Studio is installed, go ahead and open the program from your applications list (Start Menu/Launchpad/Desktop).  
  
*Note*: if R-Studio does not open, one common reason is that you have installed multiple
versions of base R and R-Studio does not know which one to access. This can happen if you
have updated R over time or uninstalled it and reinstalled it in the past. To remedy this
situation, follow the instructions in the Appendix of this document. If that doesn’t work, contact Anthony at `clairmont@ucsb.edu`.

+ You may be asked whether you want to install “Command Line Developer Tools.” R-Studio
needs these to function, so say “Install” and “Agree” to the terms.
When R-Studio is done installing its necessary tools, it is read to use! That’s all you need to do for now.

## Appendix: Troubleshooting R-Studio installation
+ On Mac, go to MacintoshHD/Library/Frameworks/R.framework/Versions and delete files
associated with the older version of R. When you try to launch R-Studio again, it should
automatically find the remaining, current version of R.
+ On Windows or Linux, this guide will show you how to tell R-Studio which version of base R to run: [https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-
RStudio-desktop](https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-
RStudio-desktop)


# The Rasch Model

set your working directory to the folder where you downloaded the CSV file.

1.Go to Session -> Set Working Directory -> Choose Directory

2. install TAM and the WrightMap package

```
install.packages("TAM")
install.packages("WrightMap")
```

Take a {#Rasch} CSV from outside of R and read it in. This means that it is something you can now work with in R. The .csv file will be read in as something called a data frame or (dataframe). This is a type of object in R, that's essentially a spreadsheet that your're used to working with. 

```{r include=FALSE }
setwd("~/DBER_Rasch")
hls <- read.csv("data/hls_dic_scale.csv")

## See the first few rows and columns
head(hls)


```

```

hls <- read.csv("hls_dic_scale.csv")
```

## See the first few rows and columns
```{r}
head(hls)
```
If you want to see view the data frame:

```
View(hls)
```

Now we call the TAM library you installed in a prior step. This tells R to use the set of functions in available in `TAM`

```{r}
library(TAM)
```

## Run the Rasch Model
This command runs a Rasch model on the selected data frame. `mod1` is an object in R that holds the data from our Rasch model (along with a lot of other information). This is the main computation step, now we just ask TAM questions about this model.

Note that the object `hls` has to contain only items and no other information. 
```{r echo=T, results='hide'}
mod1 <- tam(hls)
```

```{r}
summary(mod1)
```


## Item Difficulties
So how difficult were those items? let’s ask TAM.
We'll extract difficulties (`xsi`) from the `mod1` object (think of `mod1` like a list). We'll access this via `indexing`. The `$` sign means, access `mod1` and extract the object `xsi` which exists in `mod1`. The command `mod1$xsi$xsi` accesses just the column `xsi`, though, you may want other information at times. 

Assign those values to a column in the environment called `ItemDiff` using `<-`

```{r}
mod1$xsi
ItemDiff <- mod1$xsi$xsi 
ItemDiff
```

## Visualize
We may want to visualize or describe the distribution of item difficulties (if you want to play with binwidth, you can).

Get Item Characteristic Curves
```{r paged.print=TRUE}
plot(mod1, export = F)
```

```{r}
hist(ItemDiff)
mean(ItemDiff)
sd(ItemDiff)
```

### Exercise: 
1. Which item is the hardest? The easiest? The closest to the mean? 
**Hint**: try to use the commands such as `max()`, `min()`.


## Person Abilities
Person abilities are also of interest. We can look at the person side of the model by computing person abilities. Compute person abilities using the `tam.wle` function and assign to an object called `Abil`. Extract person abilities ($\theta_p$) from `Abil` and create an object in the `environment` called `PersonAbility` which will essentially be a column vector. **Note**: You may want more information than this at times (such as standard errors) so you may not always want to subset this way.

```{r}

#generates a data frame
Abil <- tam.wle(mod1)
```
See the first few rows of Abil. Notice you get:

1. `pid`: person id assigned by TAM.
2. `N.items`: Number of items the person was given (this becomes interesting when you have linked test forms where students may not all see the same number of items)
3. `PersonScores`: Number of items the student got right/selected an option for in the survey case. 
4. `PersonMax`: Max that person could have gotten right/selected an option for
5. `theta`: estimated person ability
6. `error`: estimated measurement error
7. `WLE.rel`: estimated person seperation reliability.

```
head(Abil)

# or

View(Abil)
```
```{r echo=FALSE}
head(Abil)
```


The column in the `Abil` data.frame corresponding to person estimates is the `theta` column. Pull out the ability estimates, theta, column if you would like, though, this creates a list. This makes it a little easier for a few basic tasks below.
```{r}

PersonAbility <- Abil$theta

```
```{r include=FALSE}
PersonAbility[1:20]
```

```{r}
# Only the first 20 shown
PersonAbility
```

You can export those estimated abilites to a .csv to save (you can also save directly in R, if you need to).

```{r}
write.csv(Abil,"HLSmod1_thetas.csv")
```

You can find the CSV file in your `Working Directory`. If you need help finding where your `working directory` is:

```{r}
getwd()
```

Descriptives for person ability
```{r}
hist(PersonAbility)
mean(PersonAbility)
sd(PersonAbility)
```

## Wright Map
To visualize the relationship between item difficulty and person ability distributions, call the WrightMap package installed previously. We'll generate a simple WrightMap. We'll clean it up a little bit by removing some elements

```{r}
library(WrightMap)
IRT.WrightMap(mod1)
IRT.WrightMap(mod1, show.thr.lab=FALSE)
```

### Exercise: 
1. Are the items appropriately targeted to the ability level of the population? 
2. Why do you think?

## Item Fit
Let's find out if the data fit the model. Use the `tam.fit` function to compute fit statistics, then display.

```{r}
fit <- tam.fit(mod1)
```


```
View(fit$itemfit)
```
```{r echo=FALSE}
fit$itemfit
```

### Exercise: 
1. Look at the `Wright Map` and the histograms of person abilities ($\theta_p$) and item difficulties ($\delta_i$). Do you think this instrument is well-targeted for this sample? 2. How might it be optimized? 
3. Relative to other items, which item fit our model the worst?


### This concludes the planned instruction in the Rasch model for our workshop. However, we've provided working code for a few other concepts in which you might be interested, including using the Rasch model with polytomous items and with multidimensional models.



# Polytomous Items {#Polytomous}

## Polytymous item types (anything with a rating Scale)
We can use the Rasch Partial Credit Model (PCM) to look at polytomous data too. We’ll start by bringing in the polytomous items from the survey. Note that TAM needs the bottom category to be coded as 0, so you may need to recode.

```{r include=FALSE}
hls2 <- read.csv("data/hls_poly_scale.csv")
```

```
hls2 <- read.csv("hls_poly_scale.csv")
```
```{r}
head(hls2)
```

```
View(hls2)
```

TAM will automatically run the PCM when our data is polytomous. There are other model-types for polytomous data such as the rating scale model. This may be more appropriate for Likert-type items. For more information, read TAM documentation or see the reference list (Bond & Fox, 2007)

```{r warning=FALSE, results='hide'}

mod2 <- tam(hls2)
```

```{r paged.print=TRUE}
summary(mod2)
```


## Item Difficulties
Now we'll get item and person characteristics just like before

```{r}
mod2$xsi
ItemDiff2 <- mod2$xsi$xsi 
ItemDiff2

#note, if you want to see this in your viewer, you can also use View().
```


## Person ability (theta) estimates

```{r}
person.ability.poly <- tam.wle(mod2)
WLEestimates.poly <- person.ability.poly$theta

head(WLEestimates.poly)


```

## Item fit statistics
```{r}
Fit.poly <- tam.fit(mod2)
```

```
Fit.poly$itemfit
```
```{r}
kable(Fit.poly$itemfit)
```
## Item characteristic curves (but now as thresholds). 
There are item characteristic curves (ICCs) for each item choice

```{r}
tthresh.poly <- tam.threshold(mod2)
plot(mod2, type = "items", export=F)
```

## Wright Map
Here’s a polytomous Wright Map

```{r}
wrightMap(WLEestimates.poly, tthresh.poly)
```

## Exercises:
1. Find an item for which Cat 3 is actually easier than the Cat 2 of another item. 
2. Find an item that has two categories that are extremely close in severity.
3.  Look at the ICC for item 14. Describe what is happening with Cat 3.

## Model Comparison

say we want to compare the two models we just ran (note, these aren't really comparable since it's a completely different model - not nested data)
```{r}
logLik(mod1)
logLik(mod2)
anova(mod1, mod2)
```

Log likelihood is the foundation of both AIC and BIC. AIC and BIC allow you to compare non-nested models while penalizing for model complexity (BIC penalizes more). In general, the model with a smaller AIC/BIC is the one that the data fit better. The two criteria sometimes disagree.



# Multidimensional Rasch Model {#multidimensional}

What if we envision something that's multidimensional? We can model that with TAM. IN fact, this is one of TAM's great strengths. Do read package documentation, though. As the number of dimensions grows, you'll have to use particular estimation methods else the model will take to long to run. 

## we start by assigning the items to a dimension using a Q-matrix

If we want to have two dimensions, we'll create a matrix with two columns. A 1 or 0 denotes whether that item belongs to dimension 1 or 2 (or both!) 
```{r include=FALSE}
library(knitr)
```
```{r}
Q <-
  matrix(c(1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0
           ,0),ncol=2)

Q
```

click on the “Q” object in the environment pane to see what we just made

## Run the multidimensional Rasch model
```{r warning=FALSE, results='hide'}
multi <- TAM::tam.mml(resp=hls, Q=Q)
```

## $\theta$ and $\delta$

```{r}

persons.multi <- tam.wle(multi)
WLEestimates.multi <- persons.multi$theta
thresholds.multi <- tam.threshold(multi)
```

```{r}
#Fit and reliabilities
Fit.multi <- tam.fit(multi)
Fit.multi$itemfit
multi$EAP.rel #EAP reliabilities
```


### Wright Map

```{r}
MDthetas.multi <-
  cbind(persons.multi$theta.Dim01,persons.multi$theta.Dim02) #one line
wrightMap(MDthetas.multi, thresholds.multi) #second line
```

Compare the first unidimensional model to the multidimensional one

```{r}
logLik(mod1)
logLik(multi)
anova(mod1, multi)
```

Alternatively, you can use `IRT.compareModels`
```{r}
compare <- CDM::IRT.compareModels(mod1, multi)
compare
summary(compare)
```
We see that model `multi` fits slightly better. However, the log likelihood difference test shows the difference is statististically significant.

```{r echo=FALSE}
compare$LRtest
```
```
compare$LRtest
```

## Exercises
1. what evidence points towards multidimensionality?
2. compare the multidimensional model to the PCM model

